name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

# Cancel in-progress runs when a new run is triggered on the same PR/branch
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  UV_CACHE_DIR: .uv_cache_ci
  PYTHONUNBUFFERED: 1

jobs:
  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.13"]

    steps:
      - name: Checkout sources
        uses: actions/checkout@v5

      - name: Download test fixtures from Google Drive
        uses: ./.github/actions/download-test-fixtures
        with:
          service-account-json: ${{ secrets.GDRIVE_SERVICE_ACCOUNT_JSON }}
          file-id: ${{ secrets.GDRIVE_FILE_ID }}
          expected-sha256: ${{ secrets.GDRIVE_FILE_SHA256 }}
          client-id: ${{ secrets.GDRIVE_CLIENT_ID }}
          client-secret: ${{ secrets.GDRIVE_CLIENT_SECRET }}

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install uv
        uses: astral-sh/setup-uv@v7

      - name: Install FFmpeg
        uses: AnimMouse/setup-ffmpeg@v1

      - name: Cache APT packages (Ubuntu)
        if: matrix.os == 'ubuntu-latest'
        uses: actions/cache@v4
        with:
          path: |
            /var/cache/apt/archives
            /var/lib/apt/lists
          key: apt-${{ runner.os }}-${{ hashFiles('.github/workflows/ci.yml') }}
          restore-keys: |
            apt-${{ runner.os }}-

      - name: Install system dependencies (Ubuntu)
        if: matrix.os == 'ubuntu-latest'
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            xvfb \
            x11-utils \
            libxkbcommon-x11-0 \
            libxcb-icccm4 \
            libxcb-image0 \
            libxcb-keysyms1 \
            libxcb-randr0 \
            libxcb-render-util0 \
            libxcb-xinerama0 \
            libxcb-xfixes0 \
            libegl1 \
            libxcb-cursor0

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: .uv_cache_ci
          key: uv-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            uv-${{ runner.os }}-${{ matrix.python-version }}-

      - name: Install dependencies
        run: |
          uv sync --dev

      - name: Cache Ruff
        if: matrix.os == 'ubuntu-latest'
        uses: actions/cache@v4
        with:
          path: ~/.cache/ruff
          key: ruff-${{ runner.os }}-${{ hashFiles('**/*.py') }}
          restore-keys: |
            ruff-${{ runner.os }}-

      - name: Lint with ruff (check)
        if: matrix.os == 'ubuntu-latest'
        continue-on-error: true
        run: |
          uv run ruff check src/ tests/ --output-format=github

      - name: Lint with ruff (format)
        if: matrix.os == 'ubuntu-latest'
        continue-on-error: true
        run: |
          uv run ruff format --check src/ tests/

      - name: Cache Mypy
        if: matrix.os == 'ubuntu-latest'
        uses: actions/cache@v4
        with:
          path: .mypy_cache
          key: mypy-${{ runner.os }}-${{ hashFiles('**/*.py', 'pyproject.toml') }}
          restore-keys: |
            mypy-${{ runner.os }}-

      - name: Type check with mypy
        if: matrix.os == 'ubuntu-latest'
        run: |
          uv run mypy src/iq_to_audio --no-error-summary

      - name: Cache Pytest
        uses: actions/cache@v4
        with:
          path: .pytest_cache
          key: pytest-${{ runner.os }}-${{ hashFiles('tests/**/*.py') }}
          restore-keys: |
            pytest-${{ runner.os }}-

      - name: Run unit tests (Linux with xvfb)
        if: matrix.os == 'ubuntu-latest'
        env:
          QT_QPA_PLATFORM: offscreen
        run: |
          xvfb-run -a uv run pytest tests/ -v --tb=short \
            --cov=iq_to_audio --cov-report=xml --cov-report=term-missing \
            --cov-fail-under=70

      - name: Run unit tests (macOS/Windows)
        if: matrix.os != 'ubuntu-latest'
        run: |
          uv run pytest tests/ -v --tb=short \
            --cov=iq_to_audio --cov-report=xml --cov-report=term-missing

      - name: Upload coverage to Codecov
        if: matrix.os == 'ubuntu-latest' && github.event_name == 'push'
        uses: codecov/codecov-action@v5
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-${{ matrix.os }}-py${{ matrix.python-version }}
          fail_ci_if_error: false

  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    permissions:
      contents: write
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout sources
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.14"

      - name: Install uv
        uses: astral-sh/setup-uv@v7

      - name: Install FFmpeg
        uses: AnimMouse/setup-ffmpeg@v1

      - name: Install minimal Qt dependencies (for pytestqt)
        run: |
          sudo apt-get update
          sudo apt-get install -y libegl1 libxkbcommon0

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: .uv_cache_ci
          key: uv-benchmark-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            uv-benchmark-${{ runner.os }}-
            uv-${{ runner.os }}-

      - name: Install dependencies
        run: |
          uv sync --dev

      - name: Run benchmarks
        continue-on-error: true
        env:
          QT_QPA_PLATFORM: offscreen
        run: |
          echo "Running performance benchmarks..."
          uv run pytest tests/test_benchmark.py \
            --benchmark-json=benchmark_results.json \
            --benchmark-columns=min,max,mean,median,stddev \
            --override-ini='addopts=-ra --strict-markers --tb=short' \
            -v || true

      - name: Display benchmark summary
        if: always()
        continue-on-error: true
        run: |
          if [ -f "benchmark_results.json" ]; then
            echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Extract benchmark count
            COUNT=$(python -c "import json; data = json.load(open('benchmark_results.json')); print(len(data.get('benchmarks', [])))" 2>/dev/null || echo "0")
            echo "Ran $COUNT benchmarks" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Show top 5 fastest and slowest benchmarks
            if [ "$COUNT" -gt "0" ]; then
              echo "### Top 5 Fastest" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              python -c "
          import json
          data = json.load(open('benchmark_results.json'))
          benchmarks = data.get('benchmarks', [])
          sorted_benchmarks = sorted(benchmarks, key=lambda x: x['stats']['mean'])[:5]
          for b in sorted_benchmarks:
              name = b['name'].replace('test_benchmark_', '')
              mean_ms = b['stats']['mean'] * 1000
              print(f'{name:45s} {mean_ms:8.2f} ms')
          " 2>/dev/null || echo "Error processing results"
              echo '```' >> $GITHUB_STEP_SUMMARY

              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### Top 5 Slowest" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              python -c "
          import json
          data = json.load(open('benchmark_results.json'))
          benchmarks = data.get('benchmarks', [])
          sorted_benchmarks = sorted(benchmarks, key=lambda x: x['stats']['mean'], reverse=True)[:5]
          for b in sorted_benchmarks:
              name = b['name'].replace('test_benchmark_', '')
              mean_ms = b['stats']['mean'] * 1000
              print(f'{name:45s} {mean_ms:8.2f} ms')
          " 2>/dev/null || echo "Error processing results"
              echo '```' >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "No benchmark results available" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Validate benchmark results
        id: validate
        run: |
          if [ -f "benchmark_results.json" ]; then
            # Check if file has actual benchmark data (not empty/minimal)
            if python -c "import json; data = json.load(open('benchmark_results.json')); exit(0 if len(data.get('benchmarks', [])) > 0 else 1)" 2>/dev/null; then
              echo "valid=true" >> $GITHUB_OUTPUT
              echo "Benchmark results are valid (found $(python -c "import json; print(len(json.load(open('benchmark_results.json')).get('benchmarks', [])))" 2>/dev/null) benchmarks)"
            else
              echo "valid=false" >> $GITHUB_OUTPUT
              echo "No benchmark data found in results"
            fi
          else
            echo "valid=false" >> $GITHUB_OUTPUT
            echo "Benchmark results file not found"
          fi

      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        if: steps.validate.outputs.valid == 'true'
        with:
          tool: 'pytest'
          output-file-path: benchmark_results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          comment-on-alert: true
          alert-threshold: '110%'
          fail-on-alert: false
          benchmark-data-dir-path: benchmarks

  pyinstaller-smoke-test:
    name: PyInstaller Build Test (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]

    steps:
      - name: Checkout sources
        uses: actions/checkout@v5

      - name: Download test fixtures from Google Drive
        uses: ./.github/actions/download-test-fixtures
        with:
          service-account-json: ${{ secrets.GDRIVE_SERVICE_ACCOUNT_JSON }}
          file-id: ${{ secrets.GDRIVE_FILE_ID }}
          expected-sha256: ${{ secrets.GDRIVE_FILE_SHA256 }}
          client-id: ${{ secrets.GDRIVE_CLIENT_ID }}
          client-secret: ${{ secrets.GDRIVE_CLIENT_SECRET }}

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.14"

      - name: Install uv
        uses: astral-sh/setup-uv@v7

      - name: Install FFmpeg
        uses: AnimMouse/setup-ffmpeg@v1

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: .uv_cache_ci
          key: uv-pyinstaller-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            uv-pyinstaller-${{ runner.os }}-
            uv-${{ runner.os }}-

      - name: Install dependencies
        run: |
          uv sync --dev

      - name: Generate platform icons
        run: |
          uv run python tools/generate_app_icons.py

      - name: Build with PyInstaller
        run: |
          uv run pyinstaller iq-to-audio.spec

      - name: Test built executable (Linux/macOS)
        if: matrix.os != 'windows-latest'
        run: |
          ./dist/iq-to-audio/iq-to-audio --help
          ./dist/iq-to-audio/iq-to-audio --version

      - name: Test built executable (Windows)
        if: matrix.os == 'windows-latest'
        run: |
          .\dist\iq-to-audio\iq-to-audio.exe --help
          .\dist\iq-to-audio\iq-to-audio.exe --version

      - name: Upload build artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: pyinstaller-logs-${{ matrix.os }}
          path: |
            build/
            *.log
